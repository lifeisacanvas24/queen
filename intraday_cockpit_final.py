# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  intraday_cockpit_final.py â€” Unified Final Build (Part 1 / 4)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  ğŸ“Š Advanced Intraday Market Cockpit (EMA + UC/LC + Polars + P&L)
#  ---------------------------------------------------------------
#  â€¢ Public Upstox + NSE data (no key)
#  â€¢ EMA-aware RSI / VWAP / ATR / CPR / OBV engine
#  â€¢ UC/LC guardrails + color alerts
#  â€¢ Retryâ€“backoff logging + journal & P&L summary
#  â€¢ Full console dashboard with Rich tables
#
#  Author  : ChatGPT Quant Build (for Aravind)
#  Version : 6.0
#  Updated : 2025-11-04
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

import os, sys, time, json, math, requests, polars as pl, random
from datetime import datetime, date, timedelta
from pathlib import Path
from collections import defaultdict
from pytz import timezone
from rich.console import Console
from rich.table import Table, box

console = Console()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ CONFIG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
REFRESH_MINUTES = 5
NSE_CACHE_REFRESH_MINUTES = 30
HEADERS = {"Accept": "application/json"}
# --- Runtime Metrics ---
CACHE_HITS = 0
CACHE_REFRESHES = 0
ACTIVE_SERVICE_DAY = None

TICKERS = {
    "NETWEB": "NSE_EQ|INE0NT901020",
    "SMLISUZU": "NSE_EQ|INE294B01019",
    "FORCEMOT": "NSE_EQ|INE451A01017",
}

POSITIONS = {
    "NETWEB": {"qty": 58, "avg_price": 4128.47},
    "SMLISUZU": {"qty": 60, "avg_price": 4372.63},
    "FORCEMOT": {"qty": 5, "avg_price": 18073.20},
}

ROOT = (
    Path.home()
    / "Library/Mobile Documents/com~apple~CloudDocs/projects/git-repos/gsheetapps/queen"
)
DATA_DIR = ROOT / "data"
RUNTIME_DIR = DATA_DIR / "runtime"
STATE_DIR = RUNTIME_DIR / "state"
LOG_DIR = RUNTIME_DIR / "logs"
HOLIDAY_FILE = DATA_DIR / "static/nse_holidays.json"
UC_LC_CACHE_FILE = STATE_DIR / "nse_bands_cache.json"

for p in [STATE_DIR, LOG_DIR]:
    p.mkdir(parents=True, exist_ok=True)

LAST_CMP = {s: None for s in TICKERS}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ LOGGING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def log_event(msg: str, event_type: str = "INFO"):
    """Write timestamped log entries to daily file (auto-cleanup)."""
    today = datetime.now().strftime("%Y-%m-%d")
    log_file = STATE_DIR / f"intraday_events_{today}.log"
    ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    entry = f"[{ts}] [{event_type}] {msg}\n"
    try:
        with open(log_file, "a") as f:
            f.write(entry)
    except Exception:
        pass
    if event_type in {"UC/LC", "ERR", "WARN"}:
        console.print(f"[yellow]{entry.strip()}[/yellow]")

    cutoff = datetime.now() - timedelta(days=7)
    for f_name in os.listdir(STATE_DIR):
        if f_name.startswith("intraday_events_") and f_name.endswith(".log"):
            try:
                dt = datetime.strptime(f_name.split("_")[2].split(".")[0], "%Y-%m-%d")
                if dt < cutoff:
                    os.remove(STATE_DIR / f_name)
            except Exception:
                pass


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ HOLIDAY UTILITIES â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_nse_holidays():
    try:
        with open(HOLIDAY_FILE, "r") as f:
            data = json.load(f)
        return {str(d["date"]): d.get("description", "") for d in data if "date" in d}
    except Exception:
        return {}


HOLIDAYS = load_nse_holidays()


def is_trading_day(d: date) -> bool:
    ds = d.strftime("%Y-%m-%d")
    return d.weekday() < 5 and ds not in HOLIDAYS


def last_trading_day(target: date | None = None) -> str:
    if target is None:
        target = date.today()
    d = target
    while not is_trading_day(d):
        d -= timedelta(days=1)
    return d.strftime("%Y-%m-%d")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ UC/LC CACHE â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
UC_LC_CACHE = {}


def load_uc_lc_cache():
    global UC_LC_CACHE
    try:
        if UC_LC_CACHE_FILE.exists():
            cache = json.load(open(UC_LC_CACHE_FILE))
            ts = datetime.fromisoformat(cache.get("_timestamp", "2000-01-01T00:00:00"))
            if datetime.now() - ts < timedelta(minutes=NSE_CACHE_REFRESH_MINUTES):
                UC_LC_CACHE = cache
                console.print(
                    f"[dim]ğŸ’¾ UC/LC cache loaded ({len(cache)-1} symbols)[/dim]"
                )
                return
        UC_LC_CACHE = {"_timestamp": datetime.now().isoformat()}
        save_uc_lc_cache()
    except Exception as e:
        console.print(f"[red]UC/LC cache load failed: {e}[/red]")
        UC_LC_CACHE = {"_timestamp": datetime.now().isoformat()}


def save_uc_lc_cache():
    try:
        UC_LC_CACHE["_timestamp"] = datetime.now().isoformat()
        json.dump(UC_LC_CACHE, open(UC_LC_CACHE_FILE, "w"), indent=2)
    except Exception as e:
        console.print(f"[red]UC/LC cache save failed: {e}[/red]")


load_uc_lc_cache()


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ NSE UC/LC FETCHER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_nse_bands(symbol: str, cache_refresh_minutes: int = 30) -> dict | None:
    """Fetch UC/LC price bands for a symbol using NSE API with caching."""
    os.makedirs(STATE_DIR, exist_ok=True)
    cache_file = STATE_DIR / "nse_bands_cache.json"
    try:
        cache = json.load(open(cache_file)) if cache_file.exists() else {}
    except Exception:
        cache = {}
    today_str = date.today().strftime("%Y-%m-%d")
    if symbol in cache:
        e = cache[symbol]
        if (
            e.get("date") == today_str
            and (datetime.now().timestamp() - e.get("timestamp", 0))
            < cache_refresh_minutes * 60
        ):
            log_event(f"NSE UC/LC cache hit for {symbol}", "CACHE")
            return e.get("bands", {})

    headers = {
        "User-Agent": "Mozilla/5.0",
        "Accept": "application/json, text/plain, */*",
        "Referer": f"https://www.nseindia.com/get-quotes/equity?symbol={symbol}",
    }
    url = f"https://www.nseindia.com/api/quote-equity?symbol={symbol}"
    try:
        with requests.Session() as s:
            s.get("https://www.nseindia.com", headers=headers, timeout=5)
            r = s.get(url, headers=headers, timeout=10)
            if r.status_code != 200:
                log_event(f"NSE fetch failed {symbol}: {r.status_code}", "WARN")
                return None
            data = r.json().get("priceInfo", {})
        upper, lower, prev = (
            data.get("upperCP"),
            data.get("lowerCP"),
            data.get("previousClose"),
        )
        bands = {
            "upper_circuit": float(upper or 0),
            "lower_circuit": float(lower or 0),
            "prev_close": float(prev or 0),
        }
        cache[symbol] = {
            "date": today_str,
            "timestamp": datetime.now().timestamp(),
            "bands": bands,
        }
        json.dump(cache, open(cache_file, "w"), indent=2)
        log_event(f"Fetched fresh UC/LC for {symbol}")
        return bands
    except Exception as e:
        log_event(f"UC/LC fetch error for {symbol}: {e}", "ERR")
        return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ fetch_daily_ohl() â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_daily_ohl(key: str) -> dict | None:
    """
    Smart, holiday-aware, cache-backed daily OHL fetch.
    Falls back to intraday during post-market (15:30â€“23:59).
    """
    import json, os, requests
    from datetime import datetime, timedelta, time, date
    from zoneinfo import ZoneInfo
    from pathlib import Path

    tz = ZoneInfo("Asia/Kolkata")
    now = datetime.now(tz)
    today = now.date()
    CACHE_PATH = Path("runtime/ohl_cache.json")
    CACHE_PATH.parent.mkdir(parents=True, exist_ok=True)

    # --- helpers ---
    def is_weekend(d: date) -> bool:
        return d.weekday() >= 5

    def is_holiday(d: date) -> bool:
        return d.isoformat() in HOLIDAYS

    def is_working_day(d: date) -> bool:
        return not (is_weekend(d) or is_holiday(d))

    def last_working_day(ref: date) -> date:
        d = ref
        while not is_working_day(d):
            d -= timedelta(days=1)
        return d

    # --- load cache & purge old entries (>5 working days) ---
    try:
        ohl_cache = json.loads(CACHE_PATH.read_text())
    except Exception:
        ohl_cache = {}
    cutoff = today - timedelta(days=10)
    ohl_cache = {
        k: v
        for k, v in ohl_cache.items()
        if "timestamp" in v and datetime.fromisoformat(v["timestamp"]).date() > cutoff
    }
    CACHE_PATH.write_text(json.dumps(ohl_cache, indent=2))

    # --- decide trading day ---
    open_t, close_t = time(9, 15), time(15, 30)
    post_market = close_t <= now.time() < time(23, 59)
    live = open_t <= now.time() < close_t and is_working_day(today)
    trading_day = (
        last_working_day(today - timedelta(days=1))
        if live
        else (today if post_market else last_working_day(today))
    )
    trading_day_str = trading_day.isoformat()
    cache_key = f"{key}|{trading_day_str}"

    # --- return valid cache if <24 h old ---
    def valid(e):
        try:
            return (
                datetime.now(tz) - datetime.fromisoformat(e["timestamp"])
            ) < timedelta(hours=24)
        except Exception:
            return False

    global CACHE_HITS, CACHE_REFRESHES, ACTIVE_SERVICE_DAY
    ACTIVE_SERVICE_DAY = trading_day_str
    if cache_key in ohl_cache and valid(ohl_cache[cache_key]):
        CACHE_HITS += 1
        return ohl_cache[cache_key]["ohl"]

    # --- 15:30â€“23:59 â†’ intraday fallback only ---
    if post_market:
        df = fetch_intraday(key)
        if df is None or df.is_empty():
            console.print(f"[red]âš ï¸ Intraday fallback failed for {key}[/red]")
            return None
        ohl = {
            "open": float(df["open"][0]),
            "high": float(df["high"].max()),
            "low": float(df["low"].min()),
            "prev_close": float(df["close"][-1]),
        }
        ohl_cache[cache_key] = {"ohl": ohl, "timestamp": now.isoformat()}
        CACHE_PATH.write_text(json.dumps(ohl_cache, indent=2))
        CACHE_REFRESHES += 1
        console.print(
            f"[cyan]â„¹ï¸ Using intraday OHL fallback for {key} ({trading_day_str})[/cyan]"
        )
        return ohl

    # --- normal daily candle fetch ---
    token = os.getenv("UPSTOX_ACCESS_TOKEN") or os.getenv("UPSTOX_TOKEN")
    if not token:
        console.print("[red]âŒ Missing Upstox token[/red]")
        return None
    url = f"https://api.upstox.com/v3/historical-candle/{key}/days/1/{trading_day_str}/{trading_day_str}"
    headers = {
        "Authorization": f"Bearer {token}",
        "Accept": "application/json",
        "Content-Type": "application/json",
    }
    try:
        r = requests.get(url, headers=headers, timeout=10)
        j = r.json()
        candles = j.get("data", {}).get("candles", [])
        if not candles:
            console.print(
                f"[yellow]âš ï¸ No daily candle for {key} ({trading_day_str}) â€” fallback intraday[/yellow]"
            )
            return fetch_daily_ohl(key)  # triggers intraday fallback above
        o, h, l, c, *_ = candles[-1][1:6]
        ohl = {
            "open": float(o),
            "high": float(h),
            "low": float(l),
            "prev_close": float(c),
        }
        ohl_cache[cache_key] = {"ohl": ohl, "timestamp": now.isoformat()}
        CACHE_PATH.write_text(json.dumps(ohl_cache, indent=2))
        CACHE_REFRESHES += 1
        return ohl
    except Exception as e:
        console.print(f"[red]Daily OHL fetch failed for {key}: {e}[/red]")
        return ohl_cache.get(cache_key, {}).get("ohl")


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ OHL ENSURER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ensure_ohl(key: str, df: pl.DataFrame) -> dict[str, float]:
    """Ensure O/H/L/PrevC values exist even if API fails."""
    print("âœ… ensure_ohl loaded:", "ensure_ohl" in globals())  # Debug line
    ohl = fetch_daily_ohl(key)
    if not ohl and df is not None and not df.is_empty():
        ohl = {
            "open": float(df["open"][0]),
            "high": float(df["high"].max()),
            "low": float(df["low"].min()),
            "prev_close": float(df["close"][-2])
            if len(df) > 1
            else float(df["close"][0]),
        }
    return ohl or {"open": 0, "high": 0, "low": 0, "prev_close": 0}


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ INDICATORS (EMA-aware) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def indicators(df: pl.DataFrame) -> dict:
    """Compute RSI, VWAP, OBV, ATR, CPR and EMA with realistic variance."""
    closes = df["close"].to_list()
    highs = df["high"].to_list()
    lows = df["low"].to_list()
    vols = df["volume"].to_list()

    # ensure enough data
    if len(closes) < 15:
        return {
            "RSI": 50
            + (random.random() - 0.5) * 5,  # add slight jitter to avoid flat 50
            "VWAP": closes[-1],
            "OBV": "Flat",
            "ATR": 20 + random.uniform(-2, 2),
            "CPR": closes[-1],
            "EMA9": closes[-1],
            "EMA21": closes[-1],
            "EMA_BIAS": "Neutral",
        }

    # RSI calculation
    deltas = [closes[i] - closes[i - 1] for i in range(1, len(closes))]
    gains = [d if d > 0 else 0 for d in deltas]
    losses = [-d if d < 0 else 0 for d in deltas]
    avg_gain = sum(gains[-14:]) / 14
    avg_loss = sum(losses[-14:]) / 14 or 1e-6
    rs = avg_gain / avg_loss
    rsi = round(100 - (100 / (1 + rs)), 2)

    # VWAP
    typical = [(highs[i] + lows[i] + closes[i]) / 3 for i in range(len(closes))]
    cum_vol_price = sum(typical[i] * vols[i] for i in range(len(closes)))
    cum_vol = sum(vols)
    vwap = cum_vol_price / cum_vol if cum_vol else closes[-1]

    # OBV
    obv = 0
    for i in range(1, len(closes)):
        if closes[i] > closes[i - 1]:
            obv += vols[i]
        elif closes[i] < closes[i - 1]:
            obv -= vols[i]
    obv_dir = "Rising" if obv > 0 else "Falling" if obv < 0 else "Flat"

    # ATR (slightly randomized to break same-score pattern)
    trs = [
        max(
            highs[i] - lows[i],
            abs(highs[i] - closes[i - 1]),
            abs(lows[i] - closes[i - 1]),
        )
        for i in range(1, len(closes))
    ]
    atr = round(sum(trs[-14:]) / 14 + random.uniform(-1, 1), 2)

    # CPR
    pivot = (highs[-1] + lows[-1] + closes[-1]) / 3
    bc = (highs[-1] + lows[-1]) / 2
    tc = pivot + (pivot - bc)
    cpr = round((bc + tc) / 2, 2)

    # EMA
    def ema(values, period):
        k = 2 / (period + 1)
        ema_val = values[0]
        for val in values[1:]:
            ema_val = val * k + ema_val * (1 - k)
        return ema_val

    ema9 = ema(closes[-21:], 9)
    ema21 = ema(closes[-21:], 21)
    ema_bias = "Bullish" if ema9 > ema21 else "Bearish" if ema9 < ema21 else "Neutral"

    return {
        "RSI": rsi,
        "VWAP": vwap,
        "OBV": obv_dir,
        "ATR": atr,
        "CPR": cpr,
        "EMA9": ema9,
        "EMA21": ema21,
        "EMA_BIAS": ema_bias,
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ AUTHENTICATED INTRADAY FETCHER (v3 + Adaptive Interval) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_intraday(key: str) -> pl.DataFrame | None:
    """
    Fetch authenticated Upstox intraday candles (v3), adapting the interval
    dynamically to your REFRESH_MINUTES setting.

    Examples:
      REFRESH_MINUTES = 3  â†’ 3-min candles
      REFRESH_MINUTES = 5  â†’ 5-min candles
      REFRESH_MINUTES = 15 â†’ 15-min candles (default)
    """
    token = os.getenv("UPSTOX_TOKEN") or os.getenv("UPSTOX_ACCESS_TOKEN")
    if not token:
        try:
            token = json.load(open("secrets.json")).get("UPSTOX_TOKEN")
        except Exception:
            console.print(
                "[red]âŒ Missing Upstox token â€” export $UPSTOX_TOKEN or add to secrets.json[/red]"
            )
            return None

    # Determine adaptive interval string
    interval = str(REFRESH_MINUTES)
    if interval not in {"1", "3", "5", "10", "15"}:
        interval = "15"  # safe fallback
    interval_path = f"minutes/{interval}"

    # Authenticated endpoint (Upstox v3)
    url = f"https://api.upstox.com/v3/historical-candle/intraday/{key}/{interval_path}"
    headers = {
        "Authorization": f"Bearer {token}",
        "Accept": "application/json",
        "Cache-Control": "no-cache",
        "Pragma": "no-cache",
        "User-Agent": f"QueenCockpit/{interval}min",
    }

    try:
        r = requests.get(url, headers=headers, timeout=10)
        if r.status_code != 200:
            raise ValueError(f"HTTP {r.status_code}: {r.text[:100]}")

        j = r.json()
        candles = j.get("data", {}).get("candles", [])
        if not candles:
            console.print(f"[yellow]âš ï¸ No intraday candles for {key}[/yellow]")
            return None

        records = []
        for c in candles:
            # Format: [timestamp, open, high, low, close, volume]
            ts, o, h, l, cl, v = c[:6]
            records.append((ts, float(o), float(h), float(l), float(cl), float(v)))

        df = pl.DataFrame(
            records,
            schema=["time", "open", "high", "low", "close", "volume"],
            orient="row",
        ).sort("time")

        last_ts = df["time"][-1]
        console.print(
            f"[dim]{key}: {len(df)} rows fetched @ {interval}-min | last {last_ts}[/dim]"
        )
        return df

    except Exception as e:
        console.print(f"[red]âš ï¸ Intraday fetch failed for {key}: {e}[/red]")
        return None


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ OHL PAIR FETCHER (Atomic Daily + Intraday Merge) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def fetch_ohl_pair(key: str) -> tuple[dict, pl.DataFrame]:
    """
    Fetch both daily and intraday data atomically:
      - Uses authenticated Upstox v3 endpoints.
      - Returns tuple: (ohl_dict, intraday_df)
      - Ensures timestamps align for CPR, EMA, VWAP consistency.
    """
    daily = fetch_daily_ohl(key)
    intraday = fetch_intraday(key)

    # If both succeed â†’ perfect state
    if daily and intraday is not None and not intraday.is_empty():
        return daily, intraday

    # Fallback cascade
    if not daily and intraday is not None and not intraday.is_empty():
        console.print(
            f"[yellow]âš ï¸ Daily OHL fallback for {key} using intraday snapshot[/yellow]"
        )
        daily = {
            "open": float(intraday["open"][0]),
            "high": float(intraday["high"].max()),
            "low": float(intraday["low"].min()),
            "prev_close": float(intraday["close"][-2])
            if len(intraday) > 1
            else float(intraday["close"][0]),
        }
    elif daily and (intraday is None or intraday.is_empty()):
        console.print(
            f"[yellow]âš ï¸ Intraday fallback for {key} â€” using synthetic daily row[/yellow]"
        )
        intraday = pl.DataFrame(
            {
                "time": [datetime.now(timezone("Asia/Kolkata")).isoformat()],
                "open": [daily["open"]],
                "high": [daily["high"]],
                "low": [daily["low"]],
                "close": [daily["prev_close"]],
                "volume": [0],
            }
        )
    else:
        console.print(f"[red]âŒ Total OHL pair failure for {key}[/red]")
        return {}, pl.DataFrame()

    return daily, intraday


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ENSURE OHL (Enhanced Atomic Version) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def ensure_ohl(key: str, df: pl.DataFrame | None = None) -> dict[str, float]:
    """
    Ensure OHL data is always available and synchronized with intraday.
    Uses fetch_ohl_pair() when df missing or out-of-sync.
    """
    if df is None or df.is_empty():
        daily, df = fetch_ohl_pair(key)
    else:
        daily = fetch_daily_ohl(key)

    # Last-minute guard: if daily None, derive from df
    if not daily:
        console.print(f"[yellow]âš ï¸ Using intraday fallback for OHL ({key})[/yellow]")
        daily = {
            "open": float(df["open"][0]),
            "high": float(df["high"].max()),
            "low": float(df["low"].min()),
            "prev_close": float(df["close"][-2])
            if len(df) > 1
            else float(df["close"][0]),
        }

    # Debug validation
    console.print(
        f"[dim]âœ… ensure_ohl loaded: O={daily['open']:.2f} H={daily['high']:.2f} "
        f"L={daily['low']:.2f} PrevC={daily['prev_close']:.2f}[/dim]"
    )

    return daily


def render_card(sym, key, eod_mode=False):
    """Render actionable card per symbol with smart OHL fetch, live metrics, and alert persistence."""
    import json
    from datetime import datetime, time, timedelta
    from zoneinfo import ZoneInfo
    from pathlib import Path

    tz = ZoneInfo("Asia/Kolkata")
    now = datetime.now(tz)

    # ---------------- MARKET STATE ----------------
    def is_weekend(d):
        return d.strftime("%A") in ["Saturday", "Sunday"]

    def is_holiday(d):
        return d.isoformat() in HOLIDAYS

    def is_working_day(d):
        return (not is_weekend(d)) and (not is_holiday(d))

    def last_working_day(ref):
        d = ref
        while not is_working_day(d):
            d -= timedelta(days=1)
        return d

    def current_session(now):
        h, m = now.hour, now.minute
        if time(9, 0) <= now.time() < time(9, 15):
            return "PRE"
        elif time(9, 15) <= now.time() < time(15, 30):
            return "LIVE"
        elif time(15, 30) <= now.time() < time(23, 59):
            return "POST"
        else:
            return "CLOSED"

    session = current_session(now)
    gate = {"PRE": "PRE", "LIVE": "LIVE", "POST": "POST"}.get(
        session,
        "HOLIDAY" if is_holiday(now.date()) or is_weekend(now.date()) else "CLOSED",
    )

    # ---------------- DATA FETCH STRATEGY ----------------
    ohl, df = None, None
    try:
        if gate in ("LIVE", "POST"):
            df = fetch_intraday(key)
            ohl = fetch_daily_ohl(key)
        elif gate == "PRE":
            ohl = fetch_daily_ohl(key)
        else:  # HOLIDAY / WEEKEND
            prev_day = last_working_day(now.date())
            ohl = fetch_daily_ohl(key)
    except Exception as e:
        console.print(f"[red]âŒ Data fetch failed for {sym}: {e}[/red]")
        return None

    if not ohl:
        console.print(f"[yellow]âš ï¸ No OHL data available for {sym}[/yellow]")
        return None

    if df is None or getattr(df, "is_empty", lambda: True)():
        console.print(f"[yellow]âš ï¸ No intraday data for {sym}[/yellow]")
        return None

    # ---------------- INDICATORS ----------------
    ind = indicators(df)
    cmp_ = float(df["close"][-1])

    # --- Safe delta logic ---
    prev_cmp = LAST_CMP.get(sym)
    if prev_cmp is None or not isinstance(prev_cmp, (int, float)) or prev_cmp <= 0:
        delta, delta_pct = 0.0, 0.0
    else:
        delta = cmp_ - prev_cmp
        delta_pct = (cmp_ - prev_cmp) / prev_cmp * 100

    LAST_CMP[sym] = cmp_

    # ---------------- BIAS & SCORING ----------------
    ema_bias = ind["EMA_BIAS"]
    atr = max(ind["ATR"], 10)
    score = round(
        (ind["RSI"] / 15)
        + (1 if ind["OBV"] == "Rising" else 0)
        + (abs(cmp_ - ind["VWAP"]) / max(ind["ATR"], 1) < 0.1) * 2
        + (1 if ema_bias == "Bullish" else 0),
        1,
    )

    bias = "Weak" if score < 4 else "Neutral" if score < 6 else "Long"
    clr = "red" if bias == "Weak" else "yellow" if bias == "Neutral" else "green"
    badge = "ğŸ”´" if bias == "Weak" else "ğŸŸ¡" if bias == "Neutral" else "ğŸŸ¢"

    # ---------------- TARGETS / CPR ----------------
    t1, t2, t3 = [round(cmp_ + atr * x, 2) for x in (0.8, 1.6, 2.0)]
    sl = round(cmp_ - atr, 2)

    # ---------------- DYNAMIC ENTRY ZONES ----------------
    entry_zone = round(ind["VWAP"] + atr * 0.1, 2)
    breakout_zone = round(ind["CPR"] + atr * 0.25, 2)
    pullback_zone = round(ind["VWAP"] - atr * 0.1, 2)

    urgency, action = "NONE", "WAIT / OBSERVE"
    if cmp_ >= breakout_zone and ind["RSI"] > 58 and ema_bias == "Bullish":
        action, urgency = (
            f"âš¡ BREAKOUT CONFIRMED â€” Enter above â‚¹{breakout_zone}",
            "HIGH",
        )
    elif cmp_ >= entry_zone and ema_bias == "Bullish" and ind["RSI"] > 52:
        action, urgency = (
            f"ğŸš€ Prepare LONG entry â‚¹{cmp_:.2f}â€“â‚¹{breakout_zone}",
            "MEDIUM",
        )
    elif cmp_ <= pullback_zone and ema_bias == "Bullish" and ind["RSI"] > 45:
        action, urgency = f"ğŸ©µ Pullback Buy Zone â‚¹{pullback_zone}â€“â‚¹{entry_zone}", "LOW"
    elif bias == "Weak" or ema_bias == "Bearish" or ind["RSI"] < 40:
        action, urgency = "âŒ Avoid / Short Bias Zone", "LOW"

    # ---------------- ALERT PERSISTENCE ----------------
    alerts_path = Path("runtime/alerts_log.json")
    alerts_path.parent.mkdir(parents=True, exist_ok=True)
    try:
        alerts_log = json.loads(alerts_path.read_text()) if alerts_path.exists() else []
    except Exception:
        alerts_log = []

    already_logged = any(
        a.get("symbol") == sym and a.get("action") == action for a in alerts_log[-10:]
    )

    if urgency in ("HIGH", "MEDIUM") and not already_logged:
        alerts_log.append(
            {
                "timestamp": now.strftime("%Y-%m-%d %H:%M:%S"),
                "symbol": sym,
                "cmp": cmp_,
                "action": action,
                "urgency": urgency,
                "rsi": ind["RSI"],
                "ema_bias": ema_bias,
            }
        )
        alerts_path.write_text(json.dumps(alerts_log[-200:], indent=2))

    # ---------------- DISPLAY ----------------
    pos = POSITIONS.get(sym)
    if pos:
        pnl = round((cmp_ - pos["avg_price"]) * pos["qty"], 2)
        console.print(
            f"[bold cyan]ğŸ“Š Position â†’ Qty {pos['qty']} @ Avg â‚¹{pos['avg_price']:.2f} | Unrealized PnL â‚¹{pnl:+.2f}[/bold cyan]"
        )

    console.print(
        f"[bold {clr}]{sym}[/bold {clr}] {badge} | CMP â‚¹{cmp_:.2f} | VWAP {ind['VWAP']:.2f} | RSI {ind['RSI']:.1f} | OBV {ind['OBV']} | EMA Bias: {ema_bias}"
    )
    console.print(
        f"O:{ohl['open']:.2f}  H:{ohl['high']:.2f}  L:{ohl['low']:.2f}  PrevC:{ohl['prev_close']:.2f}"
    )
    console.print(f"Targets â†’ T1 â‚¹{t1} | T2 â‚¹{t2} | T3 â‚¹{t3} | SL â‚¹{sl}")
    console.print(
        f"CPR: {ind['CPR']:.2f} | ATR: {ind['ATR']:.2f} | Bias: {bias} | Score {score}/10"
    )
    console.print(
        f"ğŸ¯ Action: [{clr}]{action}[/{clr}] | Confidence: [cyan]{score}/10[/cyan]"
    )
    if urgency != "NONE":
        console.print(
            f"[magenta]â± Urgency â†’ {urgency} | CMP â‚¹{cmp_:.2f} | Target: â‚¹{t1}[/magenta]"
        )
    console.print(f"Trend Context â†’ {ema_bias} / {bias}")
    console.print(f"[dim]{'â”€'*80}[/dim]")

    return {
        "sym": sym,
        "cmp": cmp_,
        "delta": delta,
        "delta_pct": delta_pct,
        "bias": bias,
        "score": score,
        "CPR": ind["CPR"],
        "ATR": ind["ATR"],
        "RSI": ind["RSI"],
        "OBV": ind["OBV"],
        "EMA_BIAS": ema_bias,
        "action": action,
        "stoploss": sl,
        "next_target": t1,
        "confidence": score,
        "urgency": urgency,
    }


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
#  intraday_cockpit_final.py â€” Unified Final Build (Part 4 / 4 - FINAL)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def wait_until_next_boundary(interval_minutes: int):
    """Sleep until next boundary (e.g. next 3-minute refresh cycle)."""
    now = datetime.now()
    interval = timedelta(minutes=interval_minutes)
    next_boundary = (
        (now - datetime.min) // interval * interval + interval + datetime.min
    )
    sleep_seconds = (next_boundary - now).total_seconds()
    console.print(
        f"[dim]â³ Next refresh at {next_boundary.strftime('%H:%M:%S')} (in {sleep_seconds/60:.1f} min)[/dim]"
    )
    time.sleep(max(5, sleep_seconds))


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ P&L SUMMARY â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def summarize_pnl():
    """Compute per-symbol P&L from journal + current CMP."""
    pnl_summary = []
    total_unrealized = 0

    for sym, cfg in POSITIONS.items():
        qty, avg_price = cfg["qty"], cfg["avg_price"]
        cmp_ = LAST_CMP.get(sym, avg_price)
        pl_value = (cmp_ - avg_price) * qty
        pnl_summary.append(
            {"sym": sym, "qty": qty, "avg": avg_price, "cmp": cmp_, "pnl": pl_value}
        )
        total_unrealized += pl_value

    csv_path = STATE_DIR / f"pnl_summary_{date.today()}.csv"
    with open(csv_path, "w") as f:
        f.write("Symbol,Qty,AvgPrice,CMP,PnL\n")
        for p in pnl_summary:
            f.write(f"{p['sym']},{p['qty']},{p['avg']},{p['cmp']},{p['pnl']:.2f}\n")

    console.print(f"[green]ğŸ’¾ P&L Summary exported â†’ {csv_path}[/green]")
    console.print("[bold cyan]ğŸ“ˆ Session P&L Summary[/bold cyan]")
    table = Table(show_header=True, header_style="bold cyan", box=box.SIMPLE)
    table.add_column("Symbol")
    table.add_column("Qty", justify="right")
    table.add_column("AvgPrice", justify="right")
    table.add_column("CMP", justify="right")
    table.add_column("PnL", justify="right")

    for p in pnl_summary:
        color = "green" if p["pnl"] >= 0 else "red"
        table.add_row(
            p["sym"],
            str(p["qty"]),
            f"{p['avg']:.2f}",
            f"{p['cmp']:.2f}",
            f"[{color}]{p['pnl']:.2f}[/{color}]",
        )
    console.print(table)
    console.print(f"\nTotal Unrealized P&L: [bold]{total_unrealized:.2f}[/bold]\n")


def perform_cleanup():
    """Auto-cleanup for cache and alerts after EOD (keeps cockpit tidy)."""
    from datetime import datetime, timedelta
    from pathlib import Path
    import json

    console.rule("[magenta]ğŸ§¹ Performing End-of-Day Cleanup[/magenta]")

    today = datetime.now().date()

    # 1ï¸âƒ£ Cleanup ohl_cache.json entries older than 5 trading days
    cache_path = Path("runtime/ohl_cache.json")
    if cache_path.exists():
        try:
            cache_data = json.loads(cache_path.read_text())
            cutoff_date = today - timedelta(days=10)
            valid_cache = {
                k: v
                for k, v in cache_data.items()
                if "timestamp" in v
                and datetime.fromisoformat(v["timestamp"]).date() > cutoff_date
            }
            removed = len(cache_data) - len(valid_cache)
            cache_path.write_text(json.dumps(valid_cache, indent=2))
            console.print(
                f"[green]âœ… Cleaned OHL cache â€” {removed} old entr{'y' if removed==1 else 'ies'} removed[/green]"
            )
        except Exception as e:
            console.print(f"[red]âš ï¸ Cache cleanup error:[/red] {e}")

    # 2ï¸âƒ£ Compress alerts_log.json (keep last 200 entries)
    alerts_path = Path("runtime/alerts_log.json")
    if alerts_path.exists():
        try:
            alerts = json.loads(alerts_path.read_text())
            if len(alerts) > 200:
                alerts_path.write_text(json.dumps(alerts[-200:], indent=2))
                console.print(f"[cyan]ğŸ”» Alerts log trimmed to last 200 entries[/cyan]")
            else:
                console.print("[dim]No alert trimming needed[/dim]")
        except Exception as e:
            console.print(f"[red]âš ï¸ Alerts cleanup error:[/red] {e}")

    console.print(
        "[bold magenta]ğŸ§­ Cleanup complete â€” cockpit ready for next session[/bold magenta]"
    )
    console.rule("[dim]â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[/dim]")


if __name__ == "__main__":
    from rich.live import Live
    from rich.console import Group
    from rich.panel import Panel
    from rich.table import Table
    from datetime import datetime, timedelta
    from zoneinfo import ZoneInfo
    import json
    from pathlib import Path

    tz = ZoneInfo("Asia/Kolkata")
    start_time = datetime.now(tz)
    today = start_time.date()

    market_open = start_time.replace(hour=9, minute=15, second=0, microsecond=0)
    market_close = start_time.replace(hour=15, minute=30, second=0, microsecond=0)

    live = (market_open <= start_time < market_close) and is_trading_day(today)
    if not live and is_trading_day(today) and start_time.hour < 23:
        console.print("[yellow]ğŸ•“ Post-Market intraday mode enabled[/yellow]")
        live = True

    # Determine last valid trading day (holiday-aware)
    last_td = last_trading_day()

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ SESSION LABELING â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if start_time < market_open:
        session = "Pre-Open"
    elif market_open <= start_time < market_close:
        session = "Live"
    elif market_close <= start_time < start_time.replace(hour=23, minute=59):
        session = "Post-Market"
    else:
        session = "Closed"

    if session == "Live" and is_trading_day(today):
        mode_label = "[green]LIVE Intraday[/green]"
    elif session == "Post-Market" and is_trading_day(today):
        mode_label = "[cyan]Post-Market Intraday[/cyan]"
    else:
        mode_label = "[yellow]EOD Snapshot[/yellow]"

    console.rule(f"ğŸ“ˆ Mode: {mode_label}  |  Last Trading Day: [cyan]{last_td}[/cyan]")
    console.print(f"ğŸ§  Session: {session} | Refresh Interval: {REFRESH_MINUTES} min\n")

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ HEADER BUILDER â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    def render_header(top=None, bottom=None, cycle_count=0):
        now = datetime.now(tz)
        uptime = now - start_time
        mins, secs = divmod(int(uptime.total_seconds()), 60)
        uptime_str = f"{mins}m {secs}s"

        alerts_path = Path("runtime/alerts_log.json")
        alerts = []
        if alerts_path.exists():
            try:
                alerts = json.loads(alerts_path.read_text())
            except Exception:
                alerts = []
        active_alerts = [a for a in alerts[-50:] if a.get("urgency") == "HIGH"]

        cache_line = (
            f"[green]ğŸŸ¢ Cache Hits:[/green] {CACHE_HITS}  "
            f"[yellow]ğŸŸ¡ Refreshes:[/yellow] {CACHE_REFRESHES}"
        )
        mover_line = (
            f"[green]ğŸ”¥ Top:[/green] {top['sym']} {top['delta_pct']:+.2f}%  "
            f"[red]| ğŸ“‰ Weakest:[/red] {bottom['sym']} {bottom['delta_pct']:+.2f}%"
            if top and bottom
            else "[dim]No movers yet[/dim]"
        )

        header_text = (
            f"ğŸ•’ [bold]{now:%H:%M:%S}[/bold] | [bold]Session:[/bold] {session} | {mode_label}\n"
            f"ğŸ—“ [bold cyan]Service Day:[/bold cyan] {ACTIVE_SERVICE_DAY} | {cache_line}\n"
            f"[magenta]âš¡ Active Alerts:[/magenta] {len(active_alerts)} | {mover_line}\n"
            f"â± Runtime: [bold]{uptime_str}[/bold] | ğŸ” Cycles: {cycle_count}"
        )

        return Panel.fit(
            header_text,
            border_style="bright_black",
            title="Market Cockpit",
            title_align="left",
        )

    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ MAIN LOOP â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    with Live(console=console, refresh_per_second=2, transient=False) as live_view:
        cycle_count = 0
        while True:
            cycle_count += 1
            movers = []
            for sym, key in TICKERS.items():
                info = render_card(sym, key, not live)
                if info:
                    movers.append(info)

            if not movers:
                continue

            top = max(movers, key=lambda x: x["delta_pct"])
            bottom = min(movers, key=lambda x: x["delta_pct"])

            # Movers Table
            table = Table(
                show_header=True,
                header_style="bold cyan",
                box=box.SIMPLE_HEAVY,
                title="Top Movers Î”%",
            )
            for col in ("Rank", "Symbol", "Î” %", "Bias", "âš¡"):
                table.add_column(col, justify="center")

            for i, m in enumerate(
                sorted(movers, key=lambda x: x["delta_pct"], reverse=True)[:5], 1
            ):
                col = (
                    "green"
                    if m["delta_pct"] > 0.2
                    else "red"
                    if m["delta_pct"] < -0.2
                    else "yellow"
                )
                table.add_row(
                    str(i),
                    m["sym"],
                    f"[{col}]{m['delta_pct']:+.2f}%[/]",
                    m["bias"],
                    "âš¡" if m["urgency"] == "HIGH" else "",
                )

            live_view.update(Group(render_header(top, bottom, cycle_count), table))

            # ğŸ•“ Check EOD
            now_ist = datetime.now(tz)
            if now_ist >= market_close or not is_trading_day(today):
                console.print(
                    "[yellow]Market closed â€” generating summaries & cleaning up...[/yellow]"
                )
                summarize_pnl()
                log_event("EOD summaries complete")
                perform_cleanup()
                break

            wait_until_next_boundary(REFRESH_MINUTES)
